
@book{goodfellow_deep_2016,
	title = {Deep Learning},
	url = {http://www.deeplearningbook.org},
	publisher = {{MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016},
	file = {[1]table-of-contents.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/692DS9NW/[1]table-of-contents.pdf:application/pdf;[2]acknowledgements.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/JBPQPTGT/[2]acknowledgements.pdf:application/pdf;[3]notation.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/AYEDPSDZ/[3]notation.pdf:application/pdf;[4]chapter-1-introduction.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/JUPBV3DS/[4]chapter-1-introduction.pdf:application/pdf;[5]part-1-basics.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/NQ4D5Z7T/[5]part-1-basics.pdf:application/pdf;[6]part-1-chapter-2.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/PMNGC586/[6]part-1-chapter-2.pdf:application/pdf;[7]part-1-chapter-3.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/GQI8UI73/[7]part-1-chapter-3.pdf:application/pdf;[8]part-1-chapter-4.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/KQ2A5HSX/[8]part-1-chapter-4.pdf:application/pdf;[9]part-1-chapter-5.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/PUPGV844/[9]part-1-chapter-5.pdf:application/pdf;[10]part-2-deep-network-modern-practices.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/3LV4ZHMW/[10]part-2-deep-network-modern-practices.pdf:application/pdf;[11]part-2-chapter-6.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/6WFYSJE4/[11]part-2-chapter-6.pdf:application/pdf;[12]part-2-chapter-7.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/SXUKIVYG/[12]part-2-chapter-7.pdf:application/pdf;[13]part-2-chapter-8.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/WHWRT8IT/[13]part-2-chapter-8.pdf:application/pdf;[14]part-2-chapter-9.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/VIIMG4IT/[14]part-2-chapter-9.pdf:application/pdf;[15]part-2-chapter-10.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/A9NK2RFI/[15]part-2-chapter-10.pdf:application/pdf;[16]part-2-chapter-11.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/PN5LHX78/[16]part-2-chapter-11.pdf:application/pdf;[17]part-2-chapter-12.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/T3IFN2T6/[17]part-2-chapter-12.pdf:application/pdf;[18]part-3-deep-learning-research.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/Z9QQTBK5/[18]part-3-deep-learning-research.pdf:application/pdf;[19]part-3-chapter-13.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/T3I9IJ2C/[19]part-3-chapter-13.pdf:application/pdf;[20]part-3-chapter-14.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/QMF6787D/[20]part-3-chapter-14.pdf:application/pdf;[21]part-3-chapter-15.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/NXWIRNKC/[21]part-3-chapter-15.pdf:application/pdf;[22]part-3-chapter-16.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/GLK87CEN/[22]part-3-chapter-16.pdf:application/pdf;[23]part-3-chapter-17.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/VGV5NCJD/[23]part-3-chapter-17.pdf:application/pdf;[24]part-3-chapter-18.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/RU89JEUE/[24]part-3-chapter-18.pdf:application/pdf;[25]part-3-chapter-19.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/INTBEZV5/[25]part-3-chapter-19.pdf:application/pdf;[26]part-3-chapter-20.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/AWJ7DCRU/[26]part-3-chapter-20.pdf:application/pdf;[27]bibliography.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/IIUTR594/[27]bibliography.pdf:application/pdf;[28]index.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/ZFHMAM9V/[28]index.pdf:application/pdf}
}

@article{baydin_automatic_2015,
	title = {Automatic differentiation in machine learning: a survey},
	url = {http://arxiv.org/abs/1502.05767},
	shorttitle = {Automatic differentiation in machine learning},
	abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation ({AD}), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. {AD} is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and {AD} have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose {AD} has been missing from the toolbox of the machine learning community, a situation slowly changing with its ongoing adoption in mainstream machine learning frameworks. We survey the intersection of {AD} and machine learning, cover applications where {AD} has direct relevance, and address the main techniques of implementation. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
	journaltitle = {{arXiv}:1502.05767 [cs]},
	author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
	urldate = {2017-10-02},
	date = {2015-02-19},
	eprinttype = {arxiv},
	eprint = {1502.05767},
	keywords = {Computer Science - Learning, 68W30, 65D25, 68T05, Computer Science - Symbolic Computation, G.1.4, I.2.6},
	file = {arXiv\:1502.05767 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/AC2FUKDX/Baydin et al. - 2015 - Automatic differentiation in machine learning a s.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/7WJSHW7K/1502.html:text/html}
}

@incollection{goldberger_neighbourhood_2005,
	title = {Neighbourhood Components Analysis},
	url = {http://papers.nips.cc/paper/2566-neighbourhood-components-analysis.pdf},
	pages = {513--520},
	booktitle = {Advances in Neural Information Processing Systems 17},
	publisher = {{MIT} Press},
	author = {Goldberger, Jacob and Hinton, Geoffrey E and Roweis, Sam T. and Salakhutdinov, Ruslan R},
	editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
	urldate = {2017-10-25},
	date = {2005},
	file = {NIPS Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/3X7VZXNQ/Goldberger et al. - 2005 - Neighbourhood Components Analysis.pdf:application/pdf;NIPS Snapshort:/home/include4eto/fast_storage/library/zotero_library/storage/UUZXEKSH/2566-neighbourhood-components-analysis.html:text/html}
}

@inproceedings{weinberger_feature_2009,
	location = {New York, {NY}, {USA}},
	title = {Feature Hashing for Large Scale Multitask Learning},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553516},
	doi = {10.1145/1553374.1553516},
	series = {{ICML} '09},
	abstract = {Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case --- multitask learning with hundreds of thousands of tasks.},
	pages = {1113--1120},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Weinberger, Kilian and Dasgupta, Anirban and Langford, John and Smola, Alex and Attenberg, Josh},
	urldate = {2017-10-25},
	date = {2009},
	file = {ACM Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/7ZZCL5UT/Weinberger et al. - 2009 - Feature Hashing for Large Scale Multitask Learning.pdf:application/pdf}
}

@article{joulin_bag_2016,
	title = {Bag of Tricks for Efficient Text Classification},
	url = {http://arxiv.org/abs/1607.01759},
	abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier {fastText} is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train {fastText} on more than one billion words in less than ten minutes using a standard multicore{\textasciitilde}{CPU}, and classify half a million sentences among{\textasciitilde}312K classes in less than a minute.},
	journaltitle = {{arXiv}:1607.01759 [cs]},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	urldate = {2017-10-25},
	date = {2016-07-06},
	eprinttype = {arxiv},
	eprint = {1607.01759},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1607.01759 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/3WAIV4DT/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/J9XBWRZM/1607.html:text/html}
}

@article{micchelli_interpolation_1986,
	title = {Interpolation of scattered data: Distance matrices and conditionally positive definite functions},
	volume = {2},
	issn = {0176-4276, 1432-0940},
	url = {https://link.springer.com/article/10.1007/BF01893414},
	doi = {10.1007/BF01893414},
	shorttitle = {Interpolation of scattered data},
	abstract = {Among other things, we prove that multiquadric surface interpolation is always solvable, thereby settling a conjecture of R. Franke.},
	pages = {11--22},
	number = {1},
	journaltitle = {Constructive Approximation},
	shortjournal = {Constr. Approx},
	author = {Micchelli, Charles A.},
	urldate = {2017-10-25},
	date = {1986-12-01},
	langid = {english},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/PW63HNP9/Micchelli - 1986 - Interpolation of scattered data Distance matrices.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/MW7HE8HP/10.html:text/html}
}

@book{lawson_solving_1995,
	title = {Solving Least Squares Problems},
	isbn = {978-0-89871-356-5},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9781611971217},
	series = {Classics in Applied Mathematics},
	abstract = {An accessible text for the study of numerical methods for solving least squares problems remains an essential part of a scientific software foundation. Feedback that we have received from practicing engineers and scientists, as well as from educators and students in numerical analysis, indicates that this book has served this purpose. We were pleased when {SIAM} decided to republish the book in their Classics in Applied Mathematics series. The main body of the book remains unchanged from the original book that was published by Prentice-Hall in 1974, with the exception of corrections to known errata. Appendix C has been edited to reflect changes in the associated software package and the software distribution method. A new Appendix D has been added, giving a brief survey of the many new developments in topics treated in the book during the period 1974–1995. Appendix D is organized into sections corresponding to the chapters of the main body of the book and includes a bibliography listing about 230 publications from 1974 to 1995.},
	pagetotal = {351},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Lawson, C. and Hanson, R.},
	urldate = {2017-10-25},
	date = {1995-01-01},
	doi = {10.1137/1.9781611971217},
	file = {Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/9D23YN6S/1.html:text/html}
}

@article{murray_differentiation_2016,
	title = {Differentiation of the Cholesky decomposition},
	url = {http://arxiv.org/abs/1602.07527},
	abstract = {We review strategies for differentiating matrix-based computations, and derive symbolic and algorithmic update rules for differentiating expressions containing the Cholesky decomposition. We recommend new `blocked' algorithms, based on differentiating the Cholesky algorithm {DPOTRF} in the {LAPACK} library, which uses `Level 3' matrix-matrix operations from {BLAS}, and so is cache-friendly and easy to parallelize. For large matrices, the resulting algorithms are the fastest way to compute Cholesky derivatives, and are an order of magnitude faster than the algorithms in common usage. In some computing environments, symbolically-derived updates are faster for small matrices than those based on differentiating Cholesky algorithms. The symbolic and algorithmic approaches can be combined to get the best of both worlds.},
	journaltitle = {{arXiv}:1602.07527 [cs, stat]},
	author = {Murray, Iain},
	urldate = {2017-10-25},
	date = {2016-02-24},
	eprinttype = {arxiv},
	eprint = {1602.07527},
	keywords = {Statistics - Computation, Computer Science - Mathematical Software},
	file = {arXiv\:1602.07527 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/XQAUCCUN/Murray - 2016 - Differentiation of the Cholesky decomposition.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/6U2VG92S/1602.html:text/html}
}

@article{costa_probabilistic_1996,
	title = {Probabilistic interpretation of feedforward network outputs, with relationships to statistical prediction of ordinal quantities},
	volume = {07},
	issn = {0129-0657},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0129065796000610},
	doi = {10.1142/S0129065796000610},
	abstract = {Several problems require the estimation of discrete random variables whose values can be put in a one-to-one ordered correspondence with a finite subset of the natural numbers. This happens whenever quantities are involved that represent integer items, or have been quantized on a fixed number of levels, or correspond to “graded” linguistic values. Here we propose a correct probabilistic approach to such kind of problems that fully exploits all the available prior knowledge about their own structure. In spite of the very stringent constraints induced in output space, the method can be directly applied to standard feed-forward networks while keeping local computation of both outputs and error signals. According to these guidelines, we devised a neural implementation of a complex image pre-processing algorithm by using very poor resolution on the computing elements in the network.},
	pages = {627--637},
	number = {5},
	journaltitle = {International Journal of Neural Systems},
	shortjournal = {Int. J. Neur. Syst.},
	author = {Costa, Mario},
	urldate = {2017-10-25},
	date = {1996-11-01},
	file = {Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/6UXQLPX7/S0129065796000610.html:text/html}
}

@article{shazeer_outrageously_2017,
	title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
	url = {http://arxiv.org/abs/1701.06538},
	shorttitle = {Outrageously Large Neural Networks},
	abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern {GPU} clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer ({MoE}), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the {MoE} to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a {MoE} with up to 137 billion parameters is applied convolutionally between stacked {LSTM} layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
	journaltitle = {{arXiv}:1701.06538 [cs, stat]},
	author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	urldate = {2017-10-25},
	date = {2017-01-23},
	eprinttype = {arxiv},
	eprint = {1701.06538},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1701.06538 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/KHTRBMAY/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-G.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/5Q456S8Q/1701.html:text/html}
}

@article{swietojanski_learning_2016,
	title = {Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation},
	volume = {24},
	issn = {2329-9290, 2329-9304},
	url = {http://arxiv.org/abs/1601.02828},
	doi = {10.1109/TASLP.2016.2560534},
	abstract = {This work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions ({LHUC}) -- a method that linearly re-combines hidden units in a speaker- or environment-dependent manner using small amounts of unsupervised adaptation data. We also extend {LHUC} to a speaker adaptive training ({SAT}) framework that leads to a more adaptable {DNN} acoustic model, working both in a speaker-dependent and a speaker-independent manner, without the requirements to maintain auxiliary speaker-dependent feature extractors or to introduce significant speaker-dependent changes to the {DNN} structure. Through a series of experiments on four different speech recognition benchmarks ({TED} talks, Switchboard, {AMI} meetings, and Aurora4) comprising 270 test speakers, we show that {LHUC} in both its test-only and {SAT} variants results in consistent word error rate reductions ranging from 5\% to 23\% relative depending on the task and the degree of mismatch between training and test data. In addition, we have investigated the effect of the amount of adaptation data per speaker, the quality of unsupervised adaptation targets, the complementarity to other adaptation techniques, one-shot adaptation, and an extension to adapting {DNNs} trained in a sequence discriminative manner.},
	pages = {1450--1463},
	number = {8},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Swietojanski, Pawel and Li, Jinyu and Renals, Steve},
	urldate = {2017-10-25},
	date = {2016-08},
	eprinttype = {arxiv},
	eprint = {1601.02828},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Sound},
	file = {arXiv\:1601.02828 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/I4BF4DU9/Swietojanski et al. - 2016 - Learning Hidden Unit Contributions for Unsupervise.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/2L75A24B/1601.html:text/html}
}

@article{he_delving_2015,
	title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on {ImageNet} Classification},
	url = {http://arxiv.org/abs/1502.01852},
	shorttitle = {Delving Deep into Rectifiers},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit ({PReLU}) that generalizes the traditional rectified unit. {PReLU} improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our {PReLU} networks ({PReLU}-nets), we achieve 4.94\% top-5 test error on the {ImageNet} 2012 classification dataset. This is a 26\% relative improvement over the {ILSVRC} 2014 winner ({GoogLeNet}, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	journaltitle = {{arXiv}:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2017-10-25},
	date = {2015-02-06},
	eprinttype = {arxiv},
	eprint = {1502.01852},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1502.01852 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/C2XEUPRR/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/L5ERZCIL/1502.html:text/html}
}

@inproceedings{freifeld_lie_2012,
	title = {Lie Bodies: A Manifold Representation of 3D Human Shape},
	isbn = {978-3-642-33717-8 978-3-642-33718-5},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-33718-5_1},
	doi = {10.1007/978-3-642-33718-5_1},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Lie Bodies},
	abstract = {Three-dimensional object shape is commonly represented in terms of deformations of a triangular mesh from an exemplar shape. Existing models, however, are based on a Euclidean representation of shape deformations. In contrast, we argue that shape has a manifold structure: For example, summing the shape deformations for two people does not necessarily yield a deformation corresponding to a valid human shape, nor does the Euclidean difference of these two deformations provide a meaningful measure of shape dissimilarity. Consequently, we define a novel manifold for shape representation, with emphasis on body shapes, using a new Lie group of deformations. This has several advantages. First we define triangle deformations exactly, removing non-physical deformations and redundant degrees of freedom common to previous methods. Second, the Riemannian structure of Lie Bodies enables a more meaningful definition of body shape similarity by measuring distance between bodies on the manifold of body shape deformations. Third, the group structure allows the valid composition of deformations. This is important for models that factor body shape deformations into multiple causes or represent shape as a linear combination of basis shapes. Finally, body shape variation is modeled using statistics on manifolds. Instead of modeling Euclidean shape variation with Principal Component Analysis we capture shape variation on the manifold using Principal Geodesic Analysis. Our experiments show consistent visual and quantitative advantages of Lie Bodies over traditional Euclidean models of shape deformation and our representation can be easily incorporated into existing methods.},
	eventtitle = {European Conference on Computer Vision},
	pages = {1--14},
	booktitle = {Computer Vision – {ECCV} 2012},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Freifeld, Oren and Black, Michael J.},
	urldate = {2017-10-25},
	date = {2012-10-07},
	langid = {english},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/GDVAGJKI/Freifeld and Black - 2012 - Lie Bodies A Manifold Representation of 3D Human .pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/V69V2MQJ/978-3-642-33718-5_1.html:text/html}
}

@article{novembre_genes_2008,
	title = {Genes mirror geography within Europe},
	volume = {456},
	rights = {© 2008 Nature Publishing Group},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v456/n7218/full/nature07331.html},
	doi = {10.1038/nature07331},
	abstract = {Understanding the genetic structure of human populations is of fundamental interest to medical, forensic and anthropological sciences. Advances in high-throughput genotyping technology have markedly improved our understanding of global patterns of human genetic variation and suggest the potential to use large samples to uncover variation among closely spaced populations. Here we characterize genetic variation in a sample of 3,000 European individuals genotyped at over half a million variable {DNA} sites in the human genome. Despite low average levels of genetic differentiation among Europeans, we find a close correspondence between genetic and geographic distances; indeed, a geographical map of Europe arises naturally as an efficient two-dimensional summary of genetic variation in Europeans. The results emphasize that when mapping the genetic basis of a disease phenotype, spurious associations can arise if genetic structure is not properly accounted for. In addition, the results are relevant to the prospects of genetic ancestry testing; an individual’s {DNA} can be used to infer their geographic origin with surprising accuracy—often to within a few hundred kilometres.},
	pages = {98--101},
	number = {7218},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Novembre, John and Johnson, Toby and Bryc, Katarzyna and Kutalik, Zoltán and Boyko, Adam R. and Auton, Adam and Indap, Amit and King, Karen S. and Bergmann, Sven and Nelson, Matthew R. and Stephens, Matthew and Bustamante, Carlos D.},
	urldate = {2017-10-25},
	date = {2008-11-06},
	langid = {english},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/8L3VYZ8W/Novembre et al. - 2008 - Genes mirror geography within Europe.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/FHH5VQE3/nature07331.html:text/html}
}

@inproceedings{nair_rectified_2010,
	title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2010_NairH10},
	eventtitle = {Proceedings of the 27th International Conference on Machine Learning ({ICML}-10)},
	pages = {807--814},
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	urldate = {2017-10-25},
	date = {2010},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/3BZUHVDV/Nair and Hinton - 2010 - Rectified Linear Units Improve Restricted Boltzman.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/M3M3MHKP/icml2010_NairH10.html:text/html}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	volume = {15},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	shorttitle = {Dropout},
	pages = {1929--1958},
	journaltitle = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	urldate = {2017-10-25},
	date = {2014},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/28B85ZGN/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/Z8FAEAGQ/srivastava14a.html:text/html}
}

@inproceedings{vincent_extracting_2008,
	location = {New York, {NY}, {USA}},
	title = {Extracting and Composing Robust Features with Denoising Autoencoders},
	isbn = {978-1-60558-205-4},
	url = {http://doi.acm.org/10.1145/1390156.1390294},
	doi = {10.1145/1390156.1390294},
	series = {{ICML} '08},
	abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
	pages = {1096--1103},
	booktitle = {Proceedings of the 25th International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	urldate = {2017-10-25},
	date = {2008},
	file = {ACM Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/D9P6X4CF/Vincent et al. - 2008 - Extracting and Composing Robust Features with Deno.pdf:application/pdf}
}

@article{klambauer_self-normalizing_2017,
	title = {Self-Normalizing Neural Networks},
	url = {http://arxiv.org/abs/1706.02515},
	abstract = {Deep Learning has revolutionized vision via convolutional neural networks ({CNNs}) and natural language processing via recurrent neural networks ({RNNs}). However, success stories of Deep Learning with standard feed-forward neural networks ({FNNs}) are rare. {FNNs} that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks ({SNNs}) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of {SNNs} automatically converge towards zero mean and unit variance. The activation function of {SNNs} are "scaled exponential linear units" ({SELUs}), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of {SNNs} allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared {SNNs} on (a) 121 tasks from the {UCI} machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard {FNNs} and other machine learning methods such as random forests and support vector machines. {SNNs} significantly outperformed all competing {FNN} methods at 121 {UCI} tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning {SNN} architectures are often very deep. Implementations are available at: github.com/bioinf-jku/{SNNs}.},
	journaltitle = {{arXiv}:1706.02515 [cs, stat]},
	author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	urldate = {2017-10-25},
	date = {2017-06-08},
	eprinttype = {arxiv},
	eprint = {1706.02515},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1706.02515 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/6ACTLI2C/Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/S8GXTZSJ/1706.html:text/html}
}

@inproceedings{riedmiller_direct_1993,
	title = {A direct adaptive method for faster backpropagation learning: the {RPROP} algorithm},
	doi = {10.1109/ICNN.1993.298623},
	shorttitle = {A direct adaptive method for faster backpropagation learning},
	abstract = {A learning algorithm for multilayer feedforward networks, {RPROP} (resilient propagation), is proposed. To overcome the inherent disadvantages of pure gradient-descent, {RPROP} performs a local adaptation of the weight-updates according to the behavior of the error function. Contrary to other adaptive techniques, the effect of the {RPROP} adaptation process is not blurred by the unforeseeable influence of the size of the derivative, but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The capabilities of {RPROP} are shown in comparison to other adaptive techniques},
	eventtitle = {{IEEE} International Conference on Neural Networks},
	pages = {586--591 vol.1},
	booktitle = {{IEEE} International Conference on Neural Networks},
	author = {Riedmiller, M. and Braun, H.},
	date = {1993},
	keywords = {Acceleration, adaptive systems, backpropagation, Backpropagation algorithms, Computer networks, Convergence, direct adaptive method, error function, faster backpropagation learning, feedforward neural nets, Feedforward systems, gradient decent type, multilayer feedforward networks, neural nets, Neurons, {RPROP} algorithm, Supervised learning, weight-updates, Writing},
	file = {IEEE Xplore Abstract Record:/home/include4eto/fast_storage/library/zotero_library/storage/RE9YDW39/298623.html:text/html;IEEE Xplore Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/ENGHEQ93/Riedmiller and Braun - 1993 - A direct adaptive method for faster backpropagatio.pdf:application/pdf}
}

@article{kingma_adam:_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2017-10-25},
	date = {2014-12-22},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1412.6980 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/FL74CP75/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/H79LHCMV/1412.html:text/html}
}

@online{_explanation_????,
	title = {An Explanation of Xavier Initialization},
	url = {http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization},
	titleaddon = {andy's blog},
	urldate = {2017-10-25},
	file = {Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/6J25DT9U/an-explanation-of-xavier-initialization.html:text/html}
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
	eventtitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = {249--256},
	booktitle = {{PMLR}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	urldate = {2017-10-25},
	date = {2010-03-31},
	langid = {english},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/SUIYXNGR/Glorot and Bengio - 2010 - Understanding the difficulty of training deep feed.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/Y39FYBW6/glorot10a.html:text/html}
}

@inproceedings{ioffe_batch_2015,
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	url = {http://proceedings.mlr.press/v37/ioffe15.html},
	shorttitle = {Batch Normalization},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the t...},
	eventtitle = {International Conference on Machine Learning},
	pages = {448--456},
	booktitle = {{PMLR}},
	author = {Ioffe, Sergey and Szegedy, Christian},
	urldate = {2017-10-25},
	date = {2015-06-01},
	langid = {english},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/V2URCCLH/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/CDWJU5GU/ioffe15.html:text/html}
}

@online{thorey_what_????,
	title = {What does the gradient flowing through batch normalization looks like ?},
	url = {http://cthorey.github.io/backpropagation/},
	abstract = {This  past week,  I  have been  working on  the  assignments from  the
Stanford                            {CS}                           class
{CS}231n: Convolutional Neural Networks for Visual Recognition. In
particular,  I spent  a few  hours  deriving a  correct expression  to
backpropagate          the           batchnorm          regularization
(Assigment 2 - Batch Normalization)
. While this post is mainly for me not to forget about what insights I
have gained  in solving this  problem, I hope  it could be  useful to
others that are struggling with back propagation.},
	author = {thorey, Clément},
	urldate = {2017-10-25},
	file = {Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/E4INRTAC/backpropagation.html:text/html}
}

@article{vincent_stacked_2010,
	title = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
	volume = {11},
	issn = {{ISSN} 1533-7928},
	url = {http://www.jmlr.org/papers/v11/vincent10a.html},
	shorttitle = {Stacked Denoising Autoencoders},
	pages = {3371--3408},
	issue = {Dec},
	journaltitle = {Journal of Machine Learning Research},
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	urldate = {2017-10-25},
	date = {2010},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/WNZY2VAP/Vincent et al. - 2010 - Stacked Denoising Autoencoders Learning Useful Re.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/5KQXEI87/vincent10a.html:text/html}
}

@article{duchi_adaptive_2011,
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	volume = {12},
	issn = {{ISSN} 1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	pages = {2121--2159},
	issue = {Jul},
	journaltitle = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	urldate = {2017-10-25},
	date = {2011},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/5XVBSL6U/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/N92YJ9HZ/duchi11a.html:text/html}
}

@article{goodfellow_qualitatively_2014,
	title = {Qualitatively characterizing neural network optimization problems},
	url = {http://arxiv.org/abs/1412.6544},
	abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
	journaltitle = {{arXiv}:1412.6544 [cs, stat]},
	author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
	urldate = {2017-10-25},
	date = {2014-12-19},
	eprinttype = {arxiv},
	eprint = {1412.6544},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1412.6544 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/LP5WVBX7/Goodfellow et al. - 2014 - Qualitatively characterizing neural network optimi.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/6HVYY3SR/1412.html:text/html}
}

@misc{_extended_????,
	title = {An extended collection of matrix derivative results for forward and reverse mode algorithmic differentiation},
	file = {NA-08-01.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/EYJFTYPP/NA-08-01.pdf:application/pdf}
}

@article{he_delving_2015-1,
	title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on {ImageNet} Classification},
	url = {http://arxiv.org/abs/1502.01852},
	shorttitle = {Delving Deep into Rectifiers},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit ({PReLU}) that generalizes the traditional rectified unit. {PReLU} improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our {PReLU} networks ({PReLU}-nets), we achieve 4.94\% top-5 test error on the {ImageNet} 2012 classification dataset. This is a 26\% relative improvement over the {ILSVRC} 2014 winner ({GoogLeNet}, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	journaltitle = {{arXiv}:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2017-10-26},
	date = {2015-02-06},
	eprinttype = {arxiv},
	eprint = {1502.01852},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1502.01852 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/PQYV4M3G/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/GAAGFQIP/1502.html:text/html}
}

@article{xu_empirical_2015,
	title = {Empirical Evaluation of Rectified Activations in Convolutional Network},
	url = {http://arxiv.org/abs/1505.00853},
	abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit ({ReLU}), leaky rectified linear unit (Leaky {ReLU}), parametric rectified linear unit ({PReLU}) and a new randomized leaky rectified linear units ({RReLU}). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in {ReLU}. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using {RReLU}, we achieved 75.68{\textbackslash}\% accuracy on {CIFAR}-100 test set without multiple test or ensemble.},
	journaltitle = {{arXiv}:1505.00853 [cs, stat]},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	urldate = {2017-10-26},
	date = {2015-05-04},
	eprinttype = {arxiv},
	eprint = {1505.00853},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1505.00853 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/QPGA3Z9V/Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in C.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/EFM9VK24/1505.html:text/html}
}

@inproceedings{lecun_comparison_1995,
	title = {Comparison of Learning Algorithms for Handwritten Digit Recognition},
	abstract = {This paper compares the performance of classifier algorithms  on a standard database of handwritten digits. We consider not raw  accuracy, but rejection, training time, recognition time, and memory  requirements.},
	pages = {53--60},
	booktitle = {International Conference on Artificial Neural Networks},
	author = {{LeCun}, Yann and Jackel, L. and Bottou, L. and Brunot, A. and Cortes, C. and Denker, J. and Drucker, H. and Guyon, I. and Müller, U. and Säckinger, E. and Simard, P. and Vapnik, V.},
	date = {1995},
	file = {Citeseer - Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/Y9RK33KB/LeCun et al. - 1995 - Comparison of Learning Algorithms for Handwritten .pdf:application/pdf;Citeseer - Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/E77T4LV8/summary.html:text/html}
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {60},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the {ILSVRC}-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {84--90},
	number = {6},
	journaltitle = {Commun. {ACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2017-11-22},
	date = {2017-05},
	file = {ACM Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/I432TLAN/Krizhevsky et al. - 2017 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf}
}

@article{cohen_emnist:_2017,
	title = {{EMNIST}: an extension of {MNIST} to handwritten letters},
	url = {http://arxiv.org/abs/1702.05373},
	shorttitle = {{EMNIST}},
	abstract = {The {MNIST} dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The {MNIST} database was derived from a larger dataset known as the {NIST} Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full {NIST} dataset, which we have called Extended {MNIST} ({EMNIST}), which follows the same conversion paradigm used to create the {MNIST} dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original {MNIST} task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted {NIST} digits and the {MNIST} digits.},
	journaltitle = {{arXiv}:1702.05373 [cs]},
	author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
	urldate = {2017-11-25},
	date = {2017-02-17},
	eprinttype = {arxiv},
	eprint = {1702.05373},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1702.05373 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/KGJVWB8U/Cohen et al. - 2017 - EMNIST an extension of MNIST to handwritten lette.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/HLJZA6HA/1702.html:text/html}
}

@article{duchi_adaptive_2011-1,
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	volume = {12},
	issn = {{ISSN} 1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	pages = {2121--2159},
	issue = {Jul},
	journaltitle = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	urldate = {2017-11-25},
	date = {2011},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/RJZ2GNUG/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/AZKKJCY7/duchi11a.html:text/html}
}

@article{zeiler_adadelta:_2012,
	title = {{ADADELTA}: An Adaptive Learning Rate Method},
	url = {http://arxiv.org/abs/1212.5701},
	shorttitle = {{ADADELTA}},
	abstract = {We present a novel per-dimension learning rate method for gradient descent called {ADADELTA}. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the {MNIST} digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
	journaltitle = {{arXiv}:1212.5701 [cs]},
	author = {Zeiler, Matthew D.},
	urldate = {2017-11-25},
	date = {2012-12-22},
	eprinttype = {arxiv},
	eprint = {1212.5701},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1212.5701 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/CDZBJUAG/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/QIFKMBBV/1212.html:text/html}
}

@article{wager_dropout_2013,
	title = {Dropout Training as Adaptive Regularization},
	url = {http://arxiv.org/abs/1307.1493},
	abstract = {Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to {AdaGrad}, an online learning algorithm, and find that a close relative of {AdaGrad} operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the {IMDB} reviews dataset.},
	journaltitle = {{arXiv}:1307.1493 [cs, stat]},
	author = {Wager, Stefan and Wang, Sida and Liang, Percy},
	urldate = {2017-11-25},
	date = {2013-07-04},
	eprinttype = {arxiv},
	eprint = {1307.1493},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Computer Science - Learning},
	file = {arXiv\:1307.1493 PDF:/home/include4eto/fast_storage/library/zotero_library/storage/5WULM5UF/Wager et al. - 2013 - Dropout Training as Adaptive Regularization.pdf:application/pdf;arXiv.org Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/9PN6KEQB/1307.html:text/html}
}

@inproceedings{viola_rapid_2001,
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on {AdaBoost}, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	eventtitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	pages = {I--511--I--518 vol.1},
	booktitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	author = {Viola, P. and Jones, M.},
	date = {2001},
	keywords = {{AdaBoost}, background regions, boosted simple feature cascade, classifiers, Detectors, face detection, Face detection, feature extraction, Filters, Focusing, image classification, image processing, image representation, Image representation, integral image, learning (artificial intelligence), machine learning, Machine learning, object detection, Object detection, object specific focus-of-attention mechanism, Pixel, rapid object detection, real-time applications, Robustness, Skin, statistical guarantees, visual object detection},
	file = {00990517.pdf:/home/include4eto/fast_storage/library/zotero_library/storage/E7RA59RN/00990517.pdf:application/pdf;IEEE Xplore Abstract Record:/home/include4eto/fast_storage/library/zotero_library/storage/U92K66I9/990517.html:text/html}
}

@inproceedings{glorot_deep_2011,
	title = {Deep Sparse Rectifier Neural Networks},
	url = {http://proceedings.mlr.press/v15/glorot11a.html},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neu...},
	eventtitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	pages = {315--323},
	booktitle = {{PMLR}},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	urldate = {2018-01-15},
	date = {2011-06-14},
	langid = {english},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/9WPEHH6H/Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/QKYBGJTU/glorot11a.html:text/html}
}

@report{rumelhart_learning_1985,
	title = {Learning Internal Representations by Error Propagation},
	url = {http://www.dtic.mil/docs/citations/ADA164453},
	abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent. Keywords: Learning; networks; Perceptrons; Adaptive systems; Learning machines; and Back propagation.},
	number = {{ICS}-8506},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	urldate = {2018-01-20},
	date = {1985-09},
	langid = {english},
	file = {Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/BHAEBYQE/Rumelhart et al. - 1985 - Learning Internal Representations by Error Propaga.pdf:application/pdf;Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/8A2R9FLI/ADA164453.html:text/html}
}

@article{tieleman_t._and_hinton_g._e._lecture_2012,
	title = {Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
	volume = {4},
	url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
	number = {2},
	journaltitle = {{COURSERA}: Neural Networks for Machine Learning},
	author = {Tieleman, Tijmen and Hinton, Geoffrey E.},
	date = {2012}
}

@incollection{prechelt_early_2012,
	title = {Early Stopping — But When?},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-35289-8_5},
	series = {Lecture Notes in Computer Science},
	abstract = {Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (“early stopping”). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4\% on average), but cost much more training time (here: about factor 4 longer on average).},
	pages = {53--67},
	booktitle = {Neural Networks: Tricks of the Trade},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Prechelt, Lutz},
	urldate = {2018-01-20},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-35289-8_5},
	file = {Snapshot:/home/include4eto/fast_storage/library/zotero_library/storage/7JU6FA9J/978-3-642-35289-8_5.html:text/html}
}

@incollection{hu_convolutional_2014,
	title = {Convolutional Neural Network Architectures for Matching Natural Language Sentences},
	url = {http://papers.nips.cc/paper/5550-convolutional-neural-network-architectures-for-matching-natural-language-sentences.pdf},
	pages = {2042--2050},
	booktitle = {Advances in Neural Information Processing Systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Hu, Baotian and Lu, Zhengdong and Li, Hang and Chen, Qingcai},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	urldate = {2018-03-12},
	date = {2014},
	file = {NIPS Full Text PDF:/home/include4eto/fast_storage/library/zotero_library/storage/JPU9D7N5/Hu et al. - 2014 - Convolutional Neural Network Architectures for Mat.pdf:application/pdf;NIPS Snapshort:/home/include4eto/fast_storage/library/zotero_library/storage/LFPRKRKL/5550-convolutional-neural-network-architectures-for-matching-natural-language-sentences.html:text/html}
}
@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  publisher={GitHub},
  howpublished={\url{https://github.com/keras-team/keras}},
}
